---
title: "R Assignment"
output: html_notebook
---
```{r}
print(birthweight$birthweight)
```
```{r}
sample_mean <- mean(birthweight$birthweight)
mean(sample_mean)
```

```{r}
sample_sd <- sd(birthweight$birthweight)
print(sample_sd)

```
# Checking normality 
```{r}
dataset_hist <- hist(birthweight$birthweight, main= 'Distribution of Birthweight')
print(dataset_hist)
dataset_qq <- qqnorm(birthweight$birthweight, main = 'Normal Q-Q Plot')
print(dataset_qq)
```
# The data from the histogram and the Q-Q plot indicates that the dataset has normality, the histogram follows the shape of a normal distribution. 

# Bootstrap test:
```{r}
library(boot)
B <- 1000 

boot_mean <- function(data, indices) {
  mean(birthweight$birthweight[indices])
}
set.seed(123) 
boot_results <- boot(birthweight$birthweight, boot_mean, R = B)

conf_int <- quantile(boot_results$t, c(0.02, 0.98), na.rm = TRUE)

cat("Bootstrap 96% CI for the mean: [", round(myboot_ci[1], 2), ",", round(myboot_ci[2], 2), "]")
```

# Bounded 96% CI for the mean 
```{r}
standard_error <- sample_sd / sqrt(188)
```
```{r}
t_value <- qt(0.98, 187)
lower_bound <- sample_mean - (t_value * standard_error)
upper_bound <- sample_mean + (t_value * standard_error)
cat(round(lower_bound, 2), round(upper_bound, 2))
```
# Calculating sample size for length of 96% CI being at most 100
```{r}
margin_of_error <- 50
t_value <- qt(0.98, df = 187)
nn <- ceiling((t_value * sample_sd / margin_of_error) ^ 2)
cat("The sample size needed to ensure that the length of the 96% CI is at most 100 is ", nn)
```

# The sample size required is 833 

# t-test
```{r}
alpha <- 0.05
claim_value <- 2800
t_result <- t.test(birthweight$birthweight, mu = claim_value, alternative = "greater", conf.level = 0.95)
p_value_t <- t_result$p.value
print(t_result)
```
# The results of this t-test indicate that the true mean is in fact larger than 2800, this was shown through the p-value which was significant 0.01357<0.05. Therefore, the null hypothesis can be rejected. 

# sign test 
```{r}
sum(nn > 2800)
sign_test <- binom.test(107, 188, p = 0.5, conf.level = 0.96, alternative = "greater")
p_value_sign <- sign_test$p.value
print(sign_test)
```
#This verifies the claim of the expert, the evidence is statistically significant, the p-value is less than 0.05 (0.03399).

#C)

# Power of the tests:to compute the power of the tests we use simulation which can calculate the amount of times that the alpha is <0.05 when repeating 1000 times. 
# t-test
```{r}
library("pwr")
mu <- 2900
sd <- sample_sd
alpha <- 0.05
d <- (mu - 2800) / sd
power <- pwr.t.test(n=nn, d=d, sig.level=alpha, type="one.sample", alternative="greater")$power
cat("The power of the one-sample t-test is", round(power, 4))
```
# The power of the t-test is 0.9936, this indicates a high power, which allows us to reject H0 correctly with an accuracy of 99.36%, the test has a high sensitivity and can detect a true difference between the true mean and hypothesized value. (This is just an extra test before doing the simulation)

# comparing both tests                                   
```{r}
set.seed(123)
B = 1000
n = n
psign = numeric(B)
pttest = numeric(B)
for (i in 1:B) {
  x = sample(birthweight$birthweight, n, replace = TRUE)
  pttest[i] = t.test(x)$p.value
  psign[i] = binom.test(sum(x > mean(birthweight$birthweight)), n, p = 0.5, alternative = "greater")$p.value
} 
sum(psign < 0.05)
sum(pttest < 0.05)

```
# The t-test has much more power than the sign test in comparison, it's extremely accurate since the simulation produced the result of 1000/1000 alpha<0.05 being produced for the t-test, whereas only 15/1000 for the sign test, which is very weak in comparison. 

# D

```{r}
n = 188
p_hat_l <- 0.25

z_alpha <- qnorm(1 - 0.5*(1 - 0.5), lower.tail = FALSE)

p_hat <- sum(birthweight$birthweight < 2600) / n
se_hat <- sqrt(p_hat*(1 - p_hat) / n)
margin_of_error <- z_alpha * se_hat

p_hat_r <- p_hat + margin_of_error
p_hat_l <- p_hat - margin_of_error

conf_level <- 1 - (2*pnorm(-abs(z_alpha/2)))
conf_interval <- c(p_hat_l, p_hat_r)

cat("The confidence interval is [", p_hat_l, ",", conf_interval[2], "] with a confidence level of", conf_level)

```
```{r}
mean_weight = sample_mean
sd_weight = sample_sd
p_hat_l <- 0.25
z_score <- qnorm(p_hat_l)
se_mean <- sd_weight / sqrt(n)
p_hat_r <- (mean_weight + (qnorm(0.975)*sd_weight/sqrt(n)))
conf_level <- p_hat_r - p_hat_l
cat("Confidence interval: [", p_hat_l, ",", p_hat_r, "]\n")
cat("Confidence level: ", conf_level, "\n")
```


